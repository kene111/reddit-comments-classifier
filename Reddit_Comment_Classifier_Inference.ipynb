{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FE21BekzN8HT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import string\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUIlWj4FOcp0",
        "outputId": "2939994e-caab-450d-96b5-054ffc7d7efd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/Finch-Health/model/best_classfier.pkl\"\n",
        "vectorizer_path = \"/content/drive/MyDrive/Finch-Health/model/vectorizer.pkl\""
      ],
      "metadata": {
        "id": "XlxvM_zPOfWQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(model_path,\"rb\") as file:\n",
        "  classifier = pickle.load(file)"
      ],
      "metadata": {
        "id": "v0zGs_eJO5Iq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(vectorizer_path,\"rb\") as file:\n",
        "  vectorizer = pickle.load(file)"
      ],
      "metadata": {
        "id": "Qzvl0ia6Sa0v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcSgSh_mQnao",
        "outputId": "7437c08a-3bf0-4586-8cee-80b2f0eeca9f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "LIemEhvEQj0V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess text data\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text) # tokenize words.\n",
        "    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words] # remove stopwords.\n",
        "    lemmed_tokens = [wordnet_lemmatizer.lemmatize(token) for token in filtered_tokens] # lemmatize tokens.\n",
        "    text =  \" \".join(lemmed_tokens)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuations.\n",
        "    text = re.sub(r'\\d', '', text) # remove numbers.\n",
        "    return text"
      ],
      "metadata": {
        "id": "vy_CzlQkPSRW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_reddit_comment(comment_list):\n",
        "  clean_comments = [preprocess_text(comment) for comment in comment_list]\n",
        "  vectorized_clean_comment = vectorizer.transform(clean_comments)\n",
        "  y_pred = classifier.predict(vectorized_clean_comment)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "zMZHVYJWRB2N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/Finch-Health/data/training_data.csv\""
      ],
      "metadata": {
        "id": "YgjTGuDzTagG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(data_path)"
      ],
      "metadata": {
        "id": "An7wH_q5TF0Q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments = data[\"comments\"].sample(n=4)\n",
        "comments_list = comments.to_list()"
      ],
      "metadata": {
        "id": "Hp6SI-mcTfIZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### labels expected: \"veterinarian\", \"other\",and \"medical_doctor\""
      ],
      "metadata": {
        "id": "IXY2OIVBXyqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = classify_reddit_comment(comments_list)"
      ],
      "metadata": {
        "id": "HuNEovs7UDcV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4x4je_UU1hi",
        "outputId": "2bcb20d0-f3fb-4f6b-b75f-82aa1e7ac0ff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['veterinarian', 'other', 'veterinarian', 'veterinarian'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jr-eH9N7XGML"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}